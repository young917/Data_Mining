{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DM_Assignment_5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T61gB3GpaIvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IweUCW57aIvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DDPG_Mu(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DDPG_Mu, self).__init__()\n",
        "        self.fc1 = nn.Linear(3, 512)\n",
        "        self.fc_mu = nn.Linear(512, 1)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=0.0001)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        mu = torch.tanh(self.fc_mu(x))*2\n",
        "        return mu\n",
        "    \n",
        "    def train(self, loss):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "class DDPG_Q(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DDPG_Q, self).__init__()\n",
        "        self.fc_a = nn.Linear(1, 64)\n",
        "        self.fc_s = nn.Linear(3, 64)\n",
        "        self.fc_1 = nn.Linear(128, 128)\n",
        "        self.fc_q = nn.Linear(128, 1)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
        "    \n",
        "    def forward(self, x, a):\n",
        "        x1 = F.relu(self.fc_a(a))\n",
        "        x2 = F.relu(self.fc_s(x))\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        x = F.relu(self.fc_1(x))\n",
        "        q = self.fc_q(x)\n",
        "        return q\n",
        "\n",
        "    \n",
        "    def train(self, loss):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RhklW4raIvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('Pendulum-v0')\n",
        "Q, Q_p, Mu, Mu_p = DDPG_Q(), DDPG_Q(), DDPG_Mu(), DDPG_Mu()\n",
        "GAMMA = 0.99 #discount factor\n",
        "BATCH_SIZE = 64 # 128\n",
        "BUFFER_SIZE = 600000 # int(1e5) 30000 #replay buffer size\n",
        "replay_buffer = [] #다른 자료구조로 바꾸어도 상관없음.(list, queue, dict 등)\n",
        "TAU = 0.0001 #1e-3 0.01 soft update parameter\n",
        "PARAMETER_NOISE_COEF = 0.0005 # 0.0005\n",
        "ITER = 100 # 1, 200000 training 함수가 호출될때 학습 iteration 횟수."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO3KeVljaIvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training():\n",
        "    ###############Put your code here############\n",
        "    # Actor와 Critic의 loss가 target 네트워크를 이용하여 잘 정의되었는가. (target 네트워크에 gradient가 전달된다면 0점)\n",
        "    def _make_var(tensor, volatile=False, requires_grad=False, dtype=torch.FloatTensor):\n",
        "        return torch.autograd.Variable(tensor, volatile=volatile, requires_grad=requires_grad).type(dtype)\n",
        "\n",
        "    for _ in range(ITER):\n",
        "        state_batch, action_batch, reward_batch, nextstate_batch, done_batch = make_minibatch()\n",
        "\n",
        "        # Prepare for the target q batch\n",
        "        next_q_batch = Q_p(_make_var(nextstate_batch, volatile=True), Mu_p(_make_var(nextstate_batch, volatile=True)))\n",
        "        next_q_batch.volatile=False\n",
        "\n",
        "        y_batch = _make_var(reward_batch) + GAMMA * _make_var(1-done_batch) * next_q_batch\n",
        "\n",
        "        # Critic update\n",
        "        critic_output = Q(_make_var(state_batch), _make_var(action_batch))\n",
        "        critic_loss = F.mse_loss(critic_output, y_batch) # nn.MSELOSS()(y_batch, critic_output)\n",
        "        Q.train(critic_loss)\n",
        "\n",
        "        # Actor update\n",
        "        actor_loss = - Q( _make_var(state_batch), Mu(_make_var(state_batch)))\n",
        "        actor_loss = actor_loss.mean()\n",
        "        Mu.train(actor_loss)\n",
        "        \n",
        "        soft_target_update(Mu, Mu_p)\n",
        "        soft_target_update(Q, Q_p)\n",
        "    #############################################\n",
        "\n",
        "def soft_target_update(model, model_p):\n",
        "    ###############Put your code here############\n",
        "    # 하이퍼파라미터 TAU에 의해 soft target update가 적절히 이루어졌는가.\n",
        "    for param, target_param in zip(model.parameters(), model_p.parameters()):\n",
        "        target_param.data.copy_(\n",
        "            param.data * TAU + target_param.data * (1.0 - TAU)\n",
        "        )\n",
        "    #############################################\n",
        "        \n",
        "def init_target_param(model, model_p):\n",
        "    ###############Put your code here############\n",
        "    model_p.load_state_dict(model.state_dict())\n",
        "    \n",
        "    # test \n",
        "    for p, q in zip(model.parameters(), model_p.parameters()):\n",
        "        assert torch.all(torch.eq(p, q))\n",
        "    #############################################\n",
        "        \n",
        "def parameter_noise(model):\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param.add_(torch.randn(param.size()) * PARAMETER_NOISE_COEF)\n",
        "            \n",
        "def store_transition(s, a, r, s_prime, done):\n",
        "    ###############Put your code here############\n",
        "    # replay_buffer에 transition data가 적절히 저장되고 삭제되었는가.\n",
        "    if len(replay_buffer) >= BUFFER_SIZE:\n",
        "        replay_buffer.pop(0)\n",
        "    replay_buffer.append((s,a,r,s_prime,done))\n",
        "    #############################################\n",
        "    \n",
        "def make_minibatch():\n",
        "    ###############Put your code here############\n",
        "    # 네트워크를 학습할 때 매 iteration 마다 Batch transition data가 replay_buffer에서 random하게 샘플링 되었는가. \n",
        "    # make shape as (batch size, dim)\n",
        "\n",
        "    sample = random.sample(replay_buffer, BATCH_SIZE)\n",
        "    reward_batch = []\n",
        "    done_batch = []\n",
        "\n",
        "    for idx, elem in enumerate(sample):\n",
        "        s, a, r, s_prime, done = elem\n",
        "        s = s.view(1, -1) ; a = a.view(1, -1) ; s_prime = s_prime.view(1,-1)\n",
        "\n",
        "        if idx == 0:\n",
        "            state_batch, action_batch, nextstate_batch = s, a, s_prime\n",
        "        else:\n",
        "            state_batch = torch.cat((state_batch, s))\n",
        "            action_batch = torch.cat((action_batch, a))\n",
        "            nextstate_batch = torch.cat((nextstate_batch, s_prime))\n",
        "\n",
        "        reward_batch.append([r])\n",
        "        done_batch.append(float(done))\n",
        "\n",
        "    reward_batch = torch.tensor(reward_batch).view(-1,1)\n",
        "    done_batch = torch.tensor(done_batch).view(-1,1)\n",
        "\n",
        "    return state_batch, action_batch, reward_batch, nextstate_batch, done_batch\n",
        "    #############################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9Hl8VoeJaIvX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2fbddd66-ccee-4a64-be38-7fbef65fc95e"
      },
      "source": [
        "reward_sum = 0.0\n",
        "reward_list = []\n",
        "init_target_param(Mu, Mu_p)\n",
        "init_target_param(Q, Q_p)\n",
        "\n",
        "for ep in range(20000):\n",
        "    observation = env.reset()\n",
        "    while True:\n",
        "        state = torch.tensor(observation, dtype=torch.float)\n",
        "        parameter_noise(Mu)\n",
        "        action = Mu(state).detach()\n",
        "        observation, reward, done, _ = env.step([action.item()])\n",
        "        reward_sum += reward\n",
        "        next_state = torch.tensor(observation, dtype=torch.float)\n",
        "        store_transition(state, action, reward, next_state, done)   \n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "    if len(replay_buffer) >= 500:\n",
        "        training()\n",
        "\n",
        "    # 최근 20 에피소드의 평균 score값이 -200 이상인가. (BUFFER_SIZE, ITER, GAMMA등 모든 하이퍼파라미터 값들은 변경 가능하다)      \n",
        "    if ep % 20 == 19:\n",
        "        print('Episode %d'%ep,', Reward mean : %f'%(reward_sum/20.0))\n",
        "        if reward_sum/20.0 > -200.0:\n",
        "            break\n",
        "        reward_sum = 0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode 19 , Reward mean : -1396.688388\n",
            "Episode 39 , Reward mean : -1327.857886\n",
            "Episode 59 , Reward mean : -1412.292782\n",
            "Episode 79 , Reward mean : -1355.379440\n",
            "Episode 99 , Reward mean : -1454.268870\n",
            "Episode 119 , Reward mean : -1468.980751\n",
            "Episode 139 , Reward mean : -1522.613394\n",
            "Episode 159 , Reward mean : -1463.612270\n",
            "Episode 179 , Reward mean : -1449.870713\n",
            "Episode 199 , Reward mean : -1442.853659\n",
            "Episode 219 , Reward mean : -1467.714472\n",
            "Episode 239 , Reward mean : -1447.718468\n",
            "Episode 259 , Reward mean : -1377.436540\n",
            "Episode 279 , Reward mean : -1077.226470\n",
            "Episode 299 , Reward mean : -1242.213303\n",
            "Episode 319 , Reward mean : -1009.013708\n",
            "Episode 339 , Reward mean : -1159.879974\n",
            "Episode 359 , Reward mean : -1178.854035\n",
            "Episode 379 , Reward mean : -1183.102687\n",
            "Episode 399 , Reward mean : -1048.369530\n",
            "Episode 419 , Reward mean : -1167.780214\n",
            "Episode 439 , Reward mean : -1217.789007\n",
            "Episode 459 , Reward mean : -1079.103879\n",
            "Episode 479 , Reward mean : -1144.566638\n",
            "Episode 499 , Reward mean : -939.370512\n",
            "Episode 519 , Reward mean : -1023.169716\n",
            "Episode 539 , Reward mean : -1033.402062\n",
            "Episode 559 , Reward mean : -943.496805\n",
            "Episode 579 , Reward mean : -898.347326\n",
            "Episode 599 , Reward mean : -835.722759\n",
            "Episode 619 , Reward mean : -1028.941519\n",
            "Episode 639 , Reward mean : -1075.137094\n",
            "Episode 659 , Reward mean : -1075.807998\n",
            "Episode 679 , Reward mean : -1038.259298\n",
            "Episode 699 , Reward mean : -920.885100\n",
            "Episode 719 , Reward mean : -992.491538\n",
            "Episode 739 , Reward mean : -934.772244\n",
            "Episode 759 , Reward mean : -840.211033\n",
            "Episode 779 , Reward mean : -766.611727\n",
            "Episode 799 , Reward mean : -883.273303\n",
            "Episode 819 , Reward mean : -943.384579\n",
            "Episode 839 , Reward mean : -826.140531\n",
            "Episode 859 , Reward mean : -924.701656\n",
            "Episode 879 , Reward mean : -727.515057\n",
            "Episode 899 , Reward mean : -895.365018\n",
            "Episode 919 , Reward mean : -632.142190\n",
            "Episode 939 , Reward mean : -551.780944\n",
            "Episode 959 , Reward mean : -612.811015\n",
            "Episode 979 , Reward mean : -564.334458\n",
            "Episode 999 , Reward mean : -597.380163\n",
            "Episode 1019 , Reward mean : -779.240583\n",
            "Episode 1039 , Reward mean : -643.475818\n",
            "Episode 1059 , Reward mean : -633.100022\n",
            "Episode 1079 , Reward mean : -576.203911\n",
            "Episode 1099 , Reward mean : -492.695957\n",
            "Episode 1119 , Reward mean : -397.240196\n",
            "Episode 1139 , Reward mean : -330.273193\n",
            "Episode 1159 , Reward mean : -266.329552\n",
            "Episode 1179 , Reward mean : -501.833737\n",
            "Episode 1199 , Reward mean : -371.081850\n",
            "Episode 1219 , Reward mean : -396.108317\n",
            "Episode 1239 , Reward mean : -343.621620\n",
            "Episode 1259 , Reward mean : -314.786796\n",
            "Episode 1279 , Reward mean : -295.885099\n",
            "Episode 1299 , Reward mean : -237.264849\n",
            "Episode 1319 , Reward mean : -296.811452\n",
            "Episode 1339 , Reward mean : -384.149365\n",
            "Episode 1359 , Reward mean : -406.587613\n",
            "Episode 1379 , Reward mean : -316.927158\n",
            "Episode 1399 , Reward mean : -272.084590\n",
            "Episode 1419 , Reward mean : -257.700857\n",
            "Episode 1439 , Reward mean : -290.236889\n",
            "Episode 1459 , Reward mean : -207.428666\n",
            "Episode 1479 , Reward mean : -333.413342\n",
            "Episode 1499 , Reward mean : -203.024165\n",
            "Episode 1519 , Reward mean : -314.120785\n",
            "Episode 1539 , Reward mean : -215.314835\n",
            "Episode 1559 , Reward mean : -322.667925\n",
            "Episode 1579 , Reward mean : -213.015798\n",
            "Episode 1599 , Reward mean : -311.167216\n",
            "Episode 1619 , Reward mean : -191.289444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAicglYNC-Ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}